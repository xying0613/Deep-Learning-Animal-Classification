{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9b130f",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT3181: Deep Learning (2021)</span>\n",
    "***\n",
    "*CE/Lecturer:* Dr **Trung Le** | trunglm@monash.edu <br/>\n",
    "*Head Tutor:* Mr **Thanh Nguyen** | thanh.nguyen4@monash.edu <br/>\n",
    "<br/>\n",
    "Department of Data Science and AI, Faculty of Information Technology, Monash University, Australia\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19633049",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  Student Information</span>\n",
    "***\n",
    "Surname: **Haw**  <br/>\n",
    "Firstname: **Xiao Ying**    <br/>\n",
    "Student ID: **29797918**    <br/>\n",
    "Email: **xhaw0001@student.monash.edu**    <br/>\n",
    "Your tutorial time: **Tuesday 3.00p.m.-5.00p.m.**    <br/>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e285d4",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 1: Theory and Knowledge Questions</span>\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 30 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e12eab",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 1.1**</span> **Activation function plays an important role in modern Deep NNs. For each of the activation function below, state its output range and find its derivative (show your steps)**\n",
    "\n",
    "<span style=\"color:red\">**(a)**</span> Sigmoid: $\\sigma(x) = \\frac{1}{1+\\text{exp}{(-x)}}$ \n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1.5 points]</span></div> \n",
    "\n",
    "<span style=\"color:red\">**(b)**</span> Tanh: $\\sigma(x) = \\frac{\\exp(x) - \\exp{(-x)}}{\\exp(x) + \\exp{(-x)}}= \\frac{1-\\exp(-2x)}{1+\\exp(-2x)}$\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1.5 points]</span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b4ed32",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**Answer for Question 1.1**</span> <br/>\n",
    "<span style=\"color:orange\">**(a)**</span> Sigmoid: $\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$, output range is 0 to 1 <br/>\n",
    "<span style=\"color:orange\">**(b)**</span> Tanh: $\\sigma'(x) = 1-\\sigma^{2}(x)$, output range is -1 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b428f91",
   "metadata": {},
   "source": [
    "***\n",
    "####  <span style=\"color:red\">**Question 1.2**</span> **Softmax activation aims to transform discriminative values to prediction probabilities. Consider a classification task with $M=4$ classes and a data example $x$ with a ground-truth label $y=2$. Assume that at the output layer of a feed-forward neural network, we obtain the logits $h^{L}=[2,-1,5,0]$.**\n",
    "<span style=\"color:red\">**(a)**</span>  What is the corresonding prediction probabilities $p(x)$?\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1.5 points]</span></div> \n",
    "\n",
    "<span style=\"color:red\">**(b)**</span>  What is the cross-entropy loss caused by the feed-forward neural network at $(x,y)$?\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1.5 points]</span></div> \n",
    "\n",
    "**You need to show both formulas and numerical results for earning full mark. Although it is optional, it is great if you show your numpy code for your computation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f446b265",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**Answer for Question 1.2**</span> <br/>\n",
    "<span style=\"color:orange\">**(a)**</span> Formula: $ p_i=\\left[ \\frac{exp(h_m)}{\\sum_{i=1}^{M}exp(h_i)} \\right] ^{M}_{m=1} $ <br/> <br/>\n",
    "$ p_1 = \\frac{\\exp(2)}{\\exp(2) + \\exp(-1) + \\exp(5) + \\exp(0)} \\approx 0.0470, $ <br/>\n",
    "$ p_2 = \\frac{\\exp(-1)}{\\exp(2) + \\exp(-1) + \\exp(5) + \\exp(0)} \\approx 0.0023, $ <br/>\n",
    "$ p_3 = \\frac{\\exp(5)}{\\exp(2) + \\exp(-1) + \\exp(5) + \\exp(0)} \\approx 0.9443, $ <br/>\n",
    "$ p_4 = \\frac{\\exp(0)}{\\exp(2) + \\exp(-1) + \\exp(5) + \\exp(0)} \\approx 0.0064 $ <br/>\n",
    "<br/>\n",
    "\n",
    "<span style=\"color:orange\">**(b)**</span> After softmax activation function, $ p=[0.0470, 0.0023, 0.9443, 0.0064] $. <br/><br/>\n",
    "$ Formula: CE(y,p) = - \\sum^{M}_{m=1}y_mlogp_m $ <br/> <br/>\n",
    "$ l(y,p) = CE([0,1,0,0][0.0470, 0.0023, 0.9443, 0.0064]) = -(0)(log0.0470) - (1)(log0.0023) - (0)(log0.9443) - (0)(log0.0064) = - log0.0023 $ <br/><br/>\n",
    "Cross-entropy loss caused by the feed-forward neural network at (ùë•,ùë¶) is 6.0748."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6125680c",
   "metadata": {},
   "source": [
    "***\n",
    "####  <span style=\"color:red\">**Question 1.3**</span> **Linear operation and element-wise activation are two building-blocks for conducting a layer in a feedforward neural network.**\n",
    "\n",
    "<span style=\"color:red\">**(a)**</span> Assume that hidden layer $1$ has value $h^1(x)= \\left[\\begin{array}{ccc}\n",
    "1.5 & 2.0 \\end{array}\\right]^T$ and the weight matrix and bias at the second layer are:\n",
    "- $W^{2}=\\left[\\begin{array}{cc}\n",
    "-1 & -1\\\\\n",
    "-1 & 1\\\\\n",
    "-1 & 0\n",
    "\\end{array}\\right]$.\n",
    "What is the value of the hidden layer $\\bar{h}^{2}(x)$ after applying *the linear operation* with the matrix $W^2$ and the bias $b^2$ over $h^1$.\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> \n",
    "\n",
    "\n",
    "<span style=\"color:red\">**(b)**</span> Assume that we apply *the ReLU activation function* at the second layer. What is the value of the hidden layer $h^2(x)$ after we apply the activation function?\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> \n",
    "\n",
    "**You need to show both formulas and numerical results for earning full mark. Although it is optional, it is great if you show your numpy code for your computation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6fffee",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**Answer for Question 1.3**</span> <br/>\n",
    "<span style=\"color:orange\">**(a)**</span>\n",
    "$\\bar{h}^{2}(x) = W^2 h^1(x) + b^2$ <br/>\n",
    "$\\bar{h}^{2}(x) = \\left[\\begin{array}{cc}\n",
    "-3.5\\\\\n",
    "0.5\\\\\n",
    "-1.5\n",
    "\\end{array}\\right]$<br/><br/>\n",
    "\n",
    "<span style=\"color:orange\">**(b)**</span> ReLu Activation function: $ReLu(x) = max(0,x)$ <br/>\n",
    "Therefore, $h^2(x) = \\left[\\begin{array}{cc}\n",
    "0\\\\\n",
    "0.5\\\\\n",
    "0\n",
    "\\end{array}\\right]$ <br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1381be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb38004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.5],\n",
       "       [ 0.5],\n",
       "       [-1.5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy code for Question 1.3(a)\n",
    "\n",
    "h1 = np.transpose([[1.5, 2.0]])\n",
    "w2 = np.array([[-1, -1],\n",
    "              [-1, 1],\n",
    "              [-1, 0]])\n",
    "b2 = np.zeros((3,1))\n",
    "\n",
    "h2_bar = w2.dot(h1) + b2\n",
    "h2_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a5fb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. ],\n",
       "       [0.5],\n",
       "       [0. ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy code for Question 1.3(b)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "h2 = relu(h2_bar)\n",
    "h2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b35b62",
   "metadata": {},
   "source": [
    "***\n",
    "####  <span style=\"color:red\">**Question 1.4**</span> **Assume that we are conducting a multilayered feedforward neural network for a regression problem to predict to real-valued $y_1, y_2$, and $y_3$. The architecture of this network ($3 (Input)\\rightarrow4(ReLU)\\rightarrow 3(Output)$) is shown in the following figure**:\n",
    "\n",
    "\n",
    "<img src=\"Figures/FeedforwardNN.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "We now feed a feature vector $x=\\left[\\begin{array}{ccc}\n",
    "1.2 & -1 & 2\\end{array}\\right]^{T}$ with ground-truth label $y=\\left[\\begin{array}{ccc} 1.5 & -1 & 2\\end{array}\\right]^{T}$ to the above network. \n",
    "\n",
    "**Forward propagation**\n",
    "\n",
    "<span style=\"color:red\">**(a)**</span>  What is the value of $\\bar{h}^{1}(x)$?\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1.5 points]</span></div>\n",
    "\n",
    "<span style=\"color:red\">**(b)**</span>  What is the value of $h^{1}(x)$?\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1.5 points]</span></div>\n",
    "\n",
    "<span style=\"color:red\">**(c)**</span>  What is the predicted value $\\hat{y}$?\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1.5 points]</span></div>\n",
    "\n",
    "<span style=\"color:red\">**(d)**</span>  Suppose that we use the L2 loss. What is the value of the L2 loss $l$?\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1.5 points]</span></div>\n",
    "\n",
    "**Backward propagation**\n",
    "\n",
    "<span style=\"color:red\">**(e)**</span> What are the derivatives $\\frac{\\partial l}{\\partial h^{2}},\\frac{\\partial l}{\\partial W^{2}}$, and $\\frac{\\partial l}{\\partial b^{2}}$? \n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[5 points]</span></div>\n",
    "\n",
    "<span style=\"color:red\">**(f)**</span> What are the derivatives $\\frac{\\partial l}{\\partial h^{1}}, \\frac{\\partial l}{\\partial \\bar{h}^{1}},\\frac{\\partial l}{\\partial W^{1}}$, and $\\frac{\\partial l}{\\partial b^{1}}$? \n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[5 points]</span></div>\n",
    "\n",
    "**SGD update**\n",
    "\n",
    "<span style=\"color:red\">**(g)**</span> Assume that we use SGD with learning rate $\\eta=0.01$ to update the model parameters. What are the values of $W^2, b^2$ and $W^1, b^1$ after updating?\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[4 points]</span></div>\n",
    "\n",
    "**You need to show both formulas and numerical results for earning full mark. Although it is optional, it is great if you show your numpy code for your computation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1870c4",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**Answer for Question 1.4**</span> <br/>\n",
    "<span style=\"color:orange\">**(a)**</span> $\\bar{h}^{1}(x) = W^1x + b^1$ <br/>\n",
    "$\\bar{h}^{1}(x) = \\left[\\begin{array}{cc}\n",
    "2.2\\\\\n",
    "4\\\\\n",
    "-0.8\\\\\n",
    "1.2\n",
    "\\end{array}\\right]$ <br/><br/>\n",
    "\n",
    "<span style=\"color:orange\">**(b)**</span> $h^1(x) = \\sigma(\\bar{h}^{1}(x))$ <br/>\n",
    "$h^1(x) = \\left[\\begin{array}{cc}\n",
    "2.2\\\\\n",
    "4\\\\\n",
    "0\\\\\n",
    "1.2\n",
    "\\end{array}\\right]$ <br/><br/>\n",
    "\n",
    "<span style=\"color:orange\">**(c)**</span> $\\bar{h}^{2}(x) = W^2h^1(x) + b^2$ <br/>\n",
    "$\\bar{h}^{2}(x) = \\left[\\begin{array}{cc}\n",
    "7.1\\\\\n",
    "1.2\\\\\n",
    "1.1\n",
    "\\end{array}\\right]$ <br/><br/>\n",
    "\n",
    "$\\hat{y} = h^2(x) = \\sigma(\\bar{h}^{2}(x))$ <br/>\n",
    "$\\hat{y} = h^2(x) = \\left[\\begin{array}{cc}\n",
    "7.1\\\\\n",
    "1.2\\\\\n",
    "1.1\n",
    "\\end{array}\\right]$ <br/><br/>\n",
    "\n",
    "<span style=\"color:orange\">**(d)**</span> L2 loss : $l(y,\\hat{y}) = l(y,h^2(x))$ <br/>\n",
    "$l(y,\\hat{y}) = \\frac{1}{2}[(y_1-h^2_1)^2 + (y_2-h^2_2)^2 + (y_3-h^2_3)^2]$ <br/>\n",
    "$l(y,\\hat{y}) = \\frac{1}{2}[(1.5-7.1)^2 + (-1-1.2)^2 + (2-1.1)^2]$ <br/>\n",
    "$l(y,\\hat{y}) = 18.505$\n",
    "<br/>\n",
    "\n",
    "<span style=\"color:orange\">**(e)**</span> $\\because l=\\frac{1}{2}(y-\\hat{y})^2$ <br/>\n",
    "$\\therefore\\frac{\\partial l}{\\partial h^{2}} = \\frac{\\partial}{\\partial h^{2}}[\\frac{1}{2}(y-\\hat{y})^2]$ <br/><br/>\n",
    "$\\because \\hat{y} = h^2 $ <br/>\n",
    "$\\therefore \\frac{\\partial l}{\\partial h^{2}} = \\frac{\\partial}{\\partial h^{2}}[\\frac{1}{2}(y-h^2)^2]$ <br/>\n",
    "$\\frac{\\partial l}{\\partial h^{2}} = \\frac{2(y-h^2)(0-1)}{2} = h^2-y$ <br/>\n",
    "$\\frac{\\partial l}{\\partial h^{2}} = \\left[\\begin{array}{cc}\n",
    "5.6\\\\\n",
    "2.2\\\\ \n",
    "-0.9\n",
    "\\end{array}\\right]^T = \\left[\\begin{array}{cc} 5.6 & 2.2 & -0.9 \\end{array}\\right]$ <br/>\n",
    "\n",
    "We know that $h^2 = W^2h^1+b^2$, so <br/>\n",
    "$\\frac{\\partial h^2}{\\partial W^2} = \\frac{\\partial}{\\partial W^2}[W^2h^1+b^2]$ ; $\\frac{\\partial h^2}{\\partial b^2} = \\frac{\\partial}{\\partial b^2}[W^2h^1+b^2]$ <br/>\n",
    "After derivation, $\\frac{\\partial h^2}{\\partial W^2} = h^1$ ; $\\frac{\\partial h^2}{\\partial b^2} = 1$ and let $g^2 = \\frac{\\partial l}{\\partial h^2}.$ <br/><br/>\n",
    "$\\frac{\\partial l}{\\partial W^2} = \\frac{\\partial l}{\\partial h^2} \\times \\frac{\\partial h^2}{\\partial W^2}$ <br/>\n",
    "$\\frac{\\partial l}{\\partial W^2} = (g^2)^T \\times (h^1)^T$<br/>\n",
    "$\\frac{\\partial l}{\\partial W^2} = \\left[\\begin{array}{cc}\n",
    "12.32 & 22.4 & 0 & 6.72\\\\\n",
    "4.84 & 8.8 & 0 & 2.64\\\\\n",
    "-1.98 & -3.6 & 0 & -1.08\n",
    "\\end{array}\\right]$ <br/>\n",
    "\n",
    "$\\frac{\\partial l}{\\partial b^2} = \\frac{\\partial l}{\\partial h^2} \\times \\frac{\\partial h^2}{\\partial b^2}$ <br/>\n",
    "$\\frac{\\partial l}{\\partial b^2} = g^2$ <br/>\n",
    "$\\frac{\\partial l}{\\partial b^2} = \\left[\\begin{array}{cc} 5.6 & 2.2 & -0.9 \\end{array}\\right]^T = \\left[\\begin{array}{cc}\n",
    "5.6\\\\\n",
    "2.2\\\\\n",
    "-0.9\n",
    "\\end{array}\\right]$ <br/>\n",
    "\n",
    "\n",
    "<span style=\"color:orange\">**(f)**</span> \n",
    "We know that $h^2 = W^2h^1+b^2$, so $\\frac{\\partial h^2}{\\partial h^1} = \\frac{\\partial}{\\partial h^1}[W^2h^1+b^2]$. <br/>\n",
    "After derivation, $\\frac{\\partial h^2}{\\partial h^1} = W^2$. <br/><br/>\n",
    "$\\frac{\\partial l}{\\partial h^{1}} = \\frac{\\partial l}{\\partial h^{2}} \\times \\frac{\\partial h^{2}}{\\partial h^{1}}$ <br/>\n",
    "$\\frac{\\partial l}{\\partial h^{1}} = g^2W^2$ <br/>\n",
    "$\\frac{\\partial l}{\\partial h^{1}} = \\left[\\begin{array}{cc} 9.3 & 4.7 & 6.9 & -2.5 \\end{array}\\right]$ <br/><br/>\n",
    "\n",
    "Let $\\frac{\\partial l}{\\partial h^{1}}$ be $g^1$.<br/>\n",
    "$\\frac{\\partial l}{\\partial \\bar{h}^{1}} = \\frac{\\partial l}{\\partial h^{1}} \\times \\frac{\\partial h^{1}}{\\partial \\bar{h}^{1}}$ <br/>\n",
    "$\\frac{\\partial l}{\\partial \\bar{h}^{1}} = g^1diag(\\sigma'(\\bar{h}^{1}))$ <br/>\n",
    "$\\frac{\\partial l}{\\partial \\bar{h}^{1}} = \\left[\\begin{array}{cc} 9.3 & 4.7 & 6.9 & -2.5 \\end{array}\\right] \\times \\left[\\begin{array}{cc} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1\\end{array}\\right]$ <br/>\n",
    "$\\frac{\\partial l}{\\partial \\bar{h}^{1}} = \\left[\\begin{array}{cc} 9.3 & 4.7 & 0 & -2.5 \\end{array}\\right]$ <br/>\n",
    "\n",
    "$\\bar{h}^{1} = W^1x+b^1$, therefore <br/>\n",
    "$\\frac{\\partial \\bar{h}^{1}}{\\partial W^{1}} = \\frac{\\partial}{\\partial W^{1}}[W^1x+b^1] = x$ ; <br/>\n",
    "$\\frac{\\partial \\bar{h}^{1}}{\\partial b^{1}} = \\frac{\\partial}{\\partial b^{1}}[W^1x+b^1] = 1$ ; <br/>\n",
    "and let $\\frac{\\partial l}{\\partial \\bar{h}^{1}}$ be $\\bar{g}^{1}$ <br/><br/>\n",
    "$\\frac{\\partial l}{\\partial W^{1}} = \\frac{\\partial l}{\\partial \\bar{h}^{1}} \\times \\frac{\\partial \\bar{h}^{1}}{\\partial W^{1}}$ <br/>\n",
    "$\\frac{\\partial l}{\\partial W^{1}} = (\\bar{g}^{1})^T(h^0)^T$ <br/>\n",
    "$\\frac{\\partial l}{\\partial W^{1}} = \\left[\\begin{array}{cc}\n",
    "11.16 & -9.3 & 18.6 \\\\\n",
    "5.64 & -4.7 & 9.4 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "-3 & 2.5 & -5\n",
    "\\end{array}\\right]$ <br/><br/>\n",
    "\n",
    "$\\frac{\\partial l}{\\partial b^{1}} = \\frac{\\partial l}{\\partial \\bar{h}^{1}} \\times \\frac{\\partial \\bar{h}^{1}}{\\partial b^{1}}$ <br/>\n",
    "$\\frac{\\partial l}{\\partial b^{1}} = \\bar{g}^{1}$ <br/>\n",
    "$\\frac{\\partial l}{\\partial b^{1}} = \\left[\\begin{array}{cc} 9.3 & 4.7 & 0 & -2.5 \\end{array}\\right]^T = \\left[\\begin{array}{cc} 9.3 \\\\ 4.7 \\\\ 0 \\\\ -2.5 \\end{array}\\right]$ <br/><br/>\n",
    "\n",
    "<span style=\"color:orange\">**(g)**</span> $W^2 = W^2 - \\eta \\frac{\\partial l}{\\partial W^2}$ <br/>\n",
    "$W^2 = \\left[\\begin{array}{cc}\n",
    "1.3768 & 0.776 & 1 & -1.0672\\\\\n",
    "-0.0484 & -0.088 & 1 & 0.9736\\\\\n",
    "-0.9802 & 1.036 & 1 & -0.9892 \\end{array}\\right]$ after SGD update. <br/><br/>\n",
    "\n",
    "$b^2 = b^2 - \\eta \\frac{\\partial l}{\\partial b^2}$ <br/>\n",
    "$b^2 = \\left[\\begin{array}{cc} 0.944 \\\\ -0.022 \\\\ 0.509 \\end{array}\\right]$ after SGD update.<br/><br/>\n",
    "\n",
    "$W^1 = W^1 - \\eta \\frac{\\partial l}{\\partial W^1}$ <br/>\n",
    "$W^1 = \\left[\\begin{array}{cc}\n",
    "0.8884 & -0.907 & 0.186\\\\\n",
    "-0.0564 & -0.953 & 0.906\\\\\n",
    "1 & 1 & -1\\\\\n",
    "1.03 & 0.975 & 1.05\\end{array}\\right]$ after SGD update.<br/><br/>\n",
    "\n",
    "$b^1 = b^1 - \\eta \\frac{\\partial l}{\\partial b^1}$ <br/><br/>\n",
    "$b^1 = \\left[\\begin{array}{cc} -0.093 \\\\ 0.953 \\\\ 1 \\\\ -0.975 \\end{array}\\right]$ after SGD update.<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda802b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1_bar:\n",
      " [[ 2.2]\n",
      " [ 4. ]\n",
      " [-0.8]\n",
      " [ 1.2]]\n",
      "\n",
      "h1:\n",
      " [[2.2]\n",
      " [4. ]\n",
      " [0. ]\n",
      " [1.2]]\n",
      "\n",
      "h2:\n",
      " [[7.1]\n",
      " [1.2]\n",
      " [1.1]]\n",
      "\n",
      "L2 loss:\n",
      " [[12.5  ]\n",
      " [ 1.125]\n",
      " [ 6.125]]\n",
      "\n",
      "g2:\n",
      " [[ 5.6  2.2 -0.9]]\n",
      "\n",
      "derivativeW2:\n",
      " [[12.32 22.4   0.    6.72]\n",
      " [ 4.84  8.8   0.    2.64]\n",
      " [-1.98 -3.6  -0.   -1.08]]\n",
      "\n",
      "g1: [[ 9.3  4.7  6.9 -2.5]]\n",
      "\n",
      "g1bar:\n",
      " [[ 9.3  4.7  0.  -2.5]]\n",
      "\n",
      "derivativeW1:\n",
      " [[11.16 -9.3  18.6 ]\n",
      " [ 5.64 -4.7   9.4 ]\n",
      " [ 0.   -0.    0.  ]\n",
      " [-3.    2.5  -5.  ]]\n",
      "\n",
      "sgdw2:\n",
      " [[ 1.3768  0.776   1.     -1.0672]\n",
      " [-0.0484 -0.088   1.      0.9736]\n",
      " [-0.9802  1.036   1.     -0.9892]]\n",
      "\n",
      "sgdb2:\n",
      " [[ 0.944]\n",
      " [-0.022]\n",
      " [ 0.509]]\n",
      "\n",
      "sgdw1:\n",
      " [[ 0.8884 -0.907  -0.186 ]\n",
      " [-0.0564 -0.953   0.906 ]\n",
      " [ 1.      1.     -1.    ]\n",
      " [ 1.03    0.975   1.05  ]]\n",
      "\n",
      "sgdb1:\n",
      " [[-0.093]\n",
      " [ 0.953]\n",
      " [ 1.   ]\n",
      " [-0.975]]\n"
     ]
    }
   ],
   "source": [
    "# Numpy code for Question 1.4\n",
    "\n",
    "x = np.array([[1.2],\n",
    "             [-1],\n",
    "             [2]])\n",
    "\n",
    "y = np.array([[1.5],\n",
    "             [-1],\n",
    "             [2]])\n",
    "\n",
    "w1 = np.array([[1,-1,0],\n",
    "              [0,-1,1],\n",
    "              [1,1,-1],\n",
    "              [1,1,1]])\n",
    "\n",
    "b1 = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [-1]])\n",
    "\n",
    "w2 = np.array([[1.5,1,1,-1],\n",
    "              [0,0,1,1],\n",
    "              [-1,1,1,-1]])\n",
    "\n",
    "b2 = np.array([[1],\n",
    "              [0],\n",
    "              [0.5]])\n",
    "\n",
    "h1_bar = w1.dot(x) + b1\n",
    "print(\"h1_bar:\\n\", h1_bar)\n",
    "\n",
    "h1 = relu(h1_bar)\n",
    "print(\"\\nh1:\\n\", h1)\n",
    "\n",
    "h2 = w2.dot(h1) + b2\n",
    "print(\"\\nh2:\\n\", h2)\n",
    "\n",
    "loss = 0.5 * (y-h2_bar)**2\n",
    "print(\"\\nL2 loss:\\n\", loss)\n",
    "\n",
    "g2 = np.transpose(h2-y)\n",
    "print(\"\\ng2:\\n\",g2)\n",
    "\n",
    "derivativeW2 = np.transpose(g2).dot(np.transpose(h1))\n",
    "print(\"\\nderivativeW2:\\n\",derivativeW2)\n",
    "\n",
    "g1 = (g2).dot(w2)\n",
    "print(\"\\ng1:\",g1)\n",
    "\n",
    "D = np.array([[1,0,0,0],\n",
    "             [0,1,0,0],\n",
    "             [0,0,0,0],\n",
    "             [0,0,0,1]])\n",
    "g1bar = g1.dot(D)\n",
    "print(\"\\ng1bar:\\n\",g1bar)\n",
    "\n",
    "derivativeW1 = np.transpose(g1bar).dot(np.transpose(x))\n",
    "print(\"\\nderivativeW1:\\n\",derivativeW1)\n",
    "\n",
    "sgdw2 = w2 - 0.01*derivativeW2\n",
    "print(\"\\nsgdw2:\\n\", sgdw2)\n",
    "\n",
    "sgdb2 = b2 - 0.01*np.transpose(g2)\n",
    "print(\"\\nsgdb2:\\n\", sgdb2)\n",
    "\n",
    "sgdw1 = w1 - 0.01*derivativeW1\n",
    "print(\"\\nsgdw1:\\n\", sgdw1)\n",
    "\n",
    "sgdb1 = b1 - 0.01*np.transpose(g1bar)\n",
    "print(\"\\nsgdb1:\\n\", sgdb1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
